"""
Gradient Boosting Model - Model 2
Author: BLESSING OMOREGIE
GitHub: Nixiestone
Repository: nyx_trial

DO NOT EDIT THIS FILE
XGBoost Gradient Boosting for price direction prediction.
"""

import numpy as np
import pandas as pd
from xgboost import XGBClassifier
from sklearn.preprocessing import StandardScaler
import joblib
from pathlib import Path
from typing import Optional, Tuple

from ..utils.logger import get_logger


class GradientBoostingModel:
    """
    XGBoost Gradient Boosting model for predicting price direction.
    Predicts: -1 (sell), 0 (neutral), 1 (buy)
    """
    
    def __init__(self, config):
        """
        Initialize Gradient Boosting model.
        
        Args:
            config: Settings object
        """
        self.config = config
        self.logger = get_logger(__name__, config.LOG_LEVEL, config.LOG_FILE_PATH)
        
        # Initialize XGBoost model
        self.model = XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1,
            eval_metric='mlogloss'
        )
        
        self.scaler = StandardScaler()
        self.is_trained = False
        self.feature_names = []
    
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Prepare advanced technical features from OHLCV data.
        
        Args:
            df: DataFrame with OHLCV data
            
        Returns:
            DataFrame with features
        """
        features = pd.DataFrame(index=df.index)
        
        # Price action features
        features['open_close_ratio'] = df['open'] / df['close']
        features['high_close_ratio'] = df['high'] / df['close']
        features['low_close_ratio'] = df['low'] / df['close']
        features['body_size'] = np.abs(df['close'] - df['open']) / df['close']
        features['upper_shadow'] = (df['high'] - np.maximum(df['open'], df['close'])) / df['close']
        features['lower_shadow'] = (np.minimum(df['open'], df['close']) - df['low']) / df['close']
        
        # Returns at different periods
        for period in [1, 2, 3, 5, 10, 20]:
            features[f'return_{period}'] = df['close'].pct_change(period)
            features[f'log_return_{period}'] = np.log(df['close'] / df['close'].shift(period))
        
        # Moving averages crossovers
        sma_5 = df['close'].rolling(5).mean()
        sma_20 = df['close'].rolling(20).mean()
        sma_50 = df['close'].rolling(50).mean()
        features['sma_5_20_cross'] = (sma_5 > sma_20).astype(int)
        features['sma_20_50_cross'] = (sma_20 > sma_50).astype(int)
        features['price_above_sma_20'] = (df['close'] > sma_20).astype(int)
        features['price_above_sma_50'] = (df['close'] > sma_50).astype(int)
        
        # Volatility ratios
        vol_10 = df['close'].rolling(10).std()
        vol_20 = df['close'].rolling(20).std()
        vol_50 = df['close'].rolling(50).std()
        features['vol_10_20_ratio'] = vol_10 / vol_20
        features['vol_20_50_ratio'] = vol_20 / vol_50
        
        # Volume analysis
        features['volume_change'] = df['volume'].pct_change()
        features['volume_ma_ratio'] = df['volume'] / df['volume'].rolling(20).mean()
        features['price_volume_trend'] = (df['close'].pct_change() * df['volume']).rolling(10).sum()
        
        # Momentum features
        features['rsi_14'] = self._calculate_rsi(df['close'], 14)
        features['rsi_7'] = self._calculate_rsi(df['close'], 7)
        features['rsi_28'] = self._calculate_rsi(df['close'], 28)
        features['rsi_divergence'] = features['rsi_14'] - features['rsi_28']
        
        # MACD features
        macd, signal, hist = self._calculate_macd(df['close'])
        features['macd'] = macd
        features['macd_signal'] = signal
        features['macd_hist'] = hist
        features['macd_cross'] = (macd > signal).astype(int)
        features['macd_hist_change'] = hist.diff()
        
        # Stochastic features
        stoch_k, stoch_d = self._calculate_stochastic(df, 14)
        features['stoch_k'] = stoch_k
        features['stoch_d'] = stoch_d
        features['stoch_cross'] = (stoch_k > stoch_d).astype(int)
        features['stoch_overbought'] = (stoch_k > 80).astype(int)
        features['stoch_oversold'] = (stoch_k < 20).astype(int)
        
        # Bollinger Bands features
        bb_upper, bb_middle, bb_lower = self._calculate_bollinger_bands(df['close'], 20)
        features['bb_width'] = (bb_upper - bb_lower) / bb_middle
        features['bb_percent'] = (df['close'] - bb_lower) / (bb_upper - bb_lower)
        features['bb_above_upper'] = (df['close'] > bb_upper).astype(int)
        features['bb_below_lower'] = (df['close'] < bb_lower).astype(int)
        
        # ATR and volatility
        features['atr_14'] = self._calculate_atr(df, 14)
        features['atr_percent'] = features['atr_14'] / df['close']
        features['high_low_range'] = (df['high'] - df['low']) / df['close']
        
        # Trend indicators
        features['adx_14'] = self._calculate_adx(df, 14)
        features['trend_strength'] = (features['adx_14'] > 25).astype(int)
        
        # Price channels
        features['donchian_upper'] = df['high'].rolling(20).max()
        features['donchian_lower'] = df['low'].rolling(20).min()
        features['donchian_position'] = (df['close'] - features['donchian_lower']) / (features['donchian_upper'] - features['donchian_lower'])
        
        # Pattern recognition
        features['doji'] = (np.abs(df['close'] - df['open']) < (df['high'] - df['low']) * 0.1).astype(int)
        features['hammer'] = ((df['close'] > df['open']) & 
                              (features['lower_shadow'] > features['body_size'] * 2)).astype(int)
        features['shooting_star'] = ((df['open'] > df['close']) & 
                                      (features['upper_shadow'] > features['body_size'] * 2)).astype(int)
        
        # Market structure
        features['higher_high'] = (df['high'] > df['high'].shift(1)).astype(int)
        features['higher_low'] = (df['low'] > df['low'].shift(1)).astype(int)
        features['lower_high'] = (df['high'] < df['high'].shift(1)).astype(int)
        features['lower_low'] = (df['low'] < df['low'].shift(1)).astype(int)
        
        # Cumulative features
        features['cum_return_10'] = df['close'].pct_change().rolling(10).sum()
        features['cum_volume_10'] = df['volume'].rolling(10).sum()
        
        # Drop NaN values
        features = features.fillna(method='bfill').fillna(0)
        
        self.feature_names = features.columns.tolist()
        
        return features
    
    def train(self, df: pd.DataFrame, labels: np.ndarray) -> dict:
        """
        Train the Gradient Boosting model.
        
        Args:
            df: DataFrame with OHLCV data
            labels: Array of labels (-1, 0, 1)
            
        Returns:
            Dictionary with training metrics
        """
        try:
            self.logger.info("Training Gradient Boosting model...")
            
            # Prepare features
            X = self.prepare_features(df)
            
            # Remove rows with NaN in labels
            valid_idx = ~np.isnan(labels)
            X = X[valid_idx]
            y = labels[valid_idx]
            
            # Remap labels to 0, 1, 2 for XGBoost
            y_mapped = y + 1
            
            # Scale features
            X_scaled = self.scaler.fit_transform(X)
            
            # Train model
            self.model.fit(X_scaled, y_mapped)
            
            # Calculate training accuracy
            train_score = self.model.score(X_scaled, y_mapped)
            
            self.is_trained = True
            
            self.logger.info(f"Gradient Boosting trained. Accuracy: {train_score:.4f}")
            
            # Feature importance
            feature_importance = pd.DataFrame({
                'feature': self.feature_names,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            self.logger.debug(f"Top 10 features:\n{feature_importance.head(10)}")
            
            return {
                'model': 'gradient_boosting',
                'accuracy': train_score,
                'n_samples': len(y),
                'n_features': X.shape[1],
                'top_features': feature_importance.head(10).to_dict('records')
            }
            
        except Exception as e:
            self.logger.exception(f"Error training Gradient Boosting: {e}")
            return {'model': 'gradient_boosting', 'error': str(e)}
    
    def predict(self, df: pd.DataFrame) -> Tuple[int, float]:
        """
        Predict price direction.
        
        Args:
            df: DataFrame with OHLCV data
            
        Returns:
            Tuple of (prediction, confidence)
        """
        if not self.is_trained:
            self.logger.warning("Model not trained, returning neutral prediction")
            return 0, 0.33
        
        try:
            # Prepare features
            X = self.prepare_features(df)
            X_scaled = self.scaler.transform(X[-1:])
            
            # Predict
            prediction_mapped = self.model.predict(X_scaled)[0]
            prediction = int(prediction_mapped) - 1  # Remap back to -1, 0, 1
            
            probabilities = self.model.predict_proba(X_scaled)[0]
            confidence = np.max(probabilities)
            
            return prediction, float(confidence)
            
        except Exception as e:
            self.logger.exception(f"Error in prediction: {e}")
            return 0, 0.0
    
    def save(self, path: Optional[str] = None):
        """Save model to disk."""
        if path is None:
            path = self.config.MODEL_SAVE_PATH / "gradient_boosting_model.joblib"
        
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        
        joblib.dump({
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'is_trained': self.is_trained
        }, path)
        
        self.logger.info(f"Model saved to {path}")
    
    def load(self, path: Optional[str] = None):
        """Load model from disk."""
        if path is None:
            path = self.config.MODEL_SAVE_PATH / "gradient_boosting_model.joblib"
        
        if not Path(path).exists():
            self.logger.warning(f"Model file not found: {path}")
            return
        
        data = joblib.load(path)
        self.model = data['model']
        self.scaler = data['scaler']
        self.feature_names = data['feature_names']
        self.is_trained = data['is_trained']
        
        self.logger.info(f"Model loaded from {path}")
    
    # Technical indicator calculation methods (shared with Random Forest)
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """Calculate RSI."""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _calculate_macd(
        self,
        prices: pd.Series,
        fast: int = 12,
        slow: int = 26,
        signal: int = 9
    ) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """Calculate MACD."""
        ema_fast = prices.ewm(span=fast).mean()
        ema_slow = prices.ewm(span=slow).mean()
        macd = ema_fast - ema_slow
        signal_line = macd.ewm(span=signal).mean()
        histogram = macd - signal_line
        return macd, signal_line, histogram
    
    def _calculate_bollinger_bands(
        self,
        prices: pd.Series,
        period: int = 20,
        std_dev: float = 2
    ) -> Tuple[pd.Series, pd.Series, pd.Series]:
        """Calculate Bollinger Bands."""
        middle = prices.rolling(window=period).mean()
        std = prices.rolling(window=period).std()
        upper = middle + (std * std_dev)
        lower = middle - (std * std_dev)
        return upper, middle, lower
    
    def _calculate_stochastic(
        self,
        df: pd.DataFrame,
        period: int = 14
    ) -> Tuple[pd.Series, pd.Series]:
        """Calculate Stochastic Oscillator."""
        low_min = df['low'].rolling(window=period).min()
        high_max = df['high'].rolling(window=period).max()
        k = 100 * (df['close'] - low_min) / (high_max - low_min)
        d = k.rolling(window=3).mean()
        return k, d
    
    def _calculate_atr(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """Calculate Average True Range."""
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift())
        low_close = np.abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1)
        true_range = np.max(ranges, axis=1)
        atr = pd.Series(true_range).rolling(period).mean()
        return atr
    
    def _calculate_adx(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """Calculate Average Directional Index."""
        high_diff = df['high'].diff()
        low_diff = -df['low'].diff()
        
        pos_dm = high_diff.where((high_diff > low_diff) & (high_diff > 0), 0)
        neg_dm = low_diff.where((low_diff > high_diff) & (low_diff > 0), 0)
        
        atr = self._calculate_atr(df, period)
        
        pos_di = 100 * (pos_dm.rolling(period).mean() / atr)
        neg_di = 100 * (neg_dm.rolling(period).mean() / atr)
        
        dx = 100 * np.abs(pos_di - neg_di) / (pos_di + neg_di)
        adx = dx.rolling(period).mean()
        
        return adx


if __name__ == "__main__":
    # Test Gradient Boosting model
    from config.settings import settings
    
    print("Testing Gradient Boosting Model...")
    
    # Create sample data
    dates = pd.date_range('2023-01-01', periods=1000, freq='1H')
    df = pd.DataFrame({
        'open': np.random.uniform(100, 110, 1000),
        'high': np.random.uniform(110, 115, 1000),
        'low': np.random.uniform(95, 100, 1000),
        'close': np.random.uniform(100, 110, 1000),
        'volume': np.random.uniform(1000, 2000, 1000)
    }, index=dates)
    
    # Create sample labels
    labels = np.random.choice([-1, 0, 1], size=1000)
    
    # Initialize and train
    model = GradientBoostingModel(settings)
    metrics = model.train(df, labels)
    
    print(f"\nTraining Metrics:")
    print(f"  Accuracy: {metrics.get('accuracy', 0):.4f}")
    print(f"  Samples: {metrics.get('n_samples', 0)}")
    print(f"  Features: {metrics.get('n_features', 0)}")
    
    # Test prediction
    prediction, confidence = model.predict(df)
    print(f"\nPrediction: {prediction}, Confidence: {confidence:.4f}")
    
    print("\nGradient Boosting Model test completed!")  